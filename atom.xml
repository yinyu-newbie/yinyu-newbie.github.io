<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://yinyu-newbie.github.io</id>
    <title>Yu Yin&apos;s Blog</title>
    <updated>2019-06-29T10:40:00.224Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://yinyu-newbie.github.io"/>
    <link rel="self" href="https://yinyu-newbie.github.io/atom.xml"/>
    <subtitle>行到水穷处，坐看云起时。</subtitle>
    <logo>https://yinyu-newbie.github.io/images/avatar.png</logo>
    <icon>https://yinyu-newbie.github.io/favicon.ico</icon>
    <rights>All rights reserved 2019, Yu Yin&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Grokking Deep Learning - 梯度下降]]></title>
        <id>https://yinyu-newbie.github.io/post/gradient_descent</id>
        <link href="https://yinyu-newbie.github.io/post/gradient_descent">
        </link>
        <updated>2019-06-29T08:19:43.000Z</updated>
        <content type="html"><![CDATA[<p>上一篇文章 <a href="https://yinyu-newbie.github.io/post/forward_propagation/">Grokking Deep Learning - 前向传播</a> 主要介绍如何使用固定参数的神经网络，对输入进行预测，得到输出。在最后留下了两个问题：1. 如何去判断神经网络输出是否合理；2. 如果输出不合理，应该采取什么措施。</p>
<h3 id="误差函数">误差函数</h3>
<p>我们使用神经网络的目的，是输入一些容易观察、采集到的数据，预测合理的结果。例如，输入一张图片，神经网络能判断图片中是否存在一只猫。因此，需要对神经网络的预测结果与目标值进行比较，看看二者之间的差距有多大，神经网络是否能够正确地进行预测。</p>
<p>下面以一简单神经网络为例：</p>
<pre><code class="language-python">knob_weight = 0.5 
input = 0.5
goal_pred = 0.8

pred = input * knob_weight

error = (pred - goal_pred) ** 2
print(error)
</code></pre>
<pre><code class="language-shell">0.30250000000000005
</code></pre>
<p>这里，误差用 <code>(pred - goal_pred) ** 2</code> 表示，主要有如下原因：</p>
<ul>
<li>平方处理得到的误差值均为非负数，当 pred 大于 goal_pred 或者 pred 小于 goal_pred 能够同等看待。另外，如果直接将两者之差作为误差，不同样本的误差值有正数、有负数，多个样本的误差值的平均值无法反应真实情况</li>
<li>pred 和 goal_pred 差距较大时，平方处理会放大两者之间的误差，而 pred 和 goal_pred 差距较小时，相对而言不会有很大的影响</li>
</ul>
<p>通过误差函数能够<strong>直观</strong>反映神经网络在不同输入样本上的表现，因为不同输入对应的预测输出不同，当误差函数返回值为 0 时，即预测输出与目标值一致。</p>
<h3 id="学习">学习</h3>
<p>上面我们知道了如何去比较预测输出和目标值的差距，那么当两者差距较大，神经网络该如何进行调整呢？</p>
<h4 id="冷热学习">冷热学习</h4>
<p>冷热学习（Hot and cold learning）是一种很容易理解的方法。假设单输入单输出神经网络的场景，当预测输出和目标值差距较大时，进行如下操作：</p>
<ol>
<li>将权重值稍微调大一点，记录在新的权重下，对应的误差</li>
<li>将权重值稍微调小一点，记录在新的权重下，对应的误差</li>
<li>如果步骤 1 的误差比步骤 2 的误差小，那么选择步骤 1 调整权重，反之，则选择步骤 2 调整权重</li>
<li>继续 1-3 更新权重，直至误差满足要求或者迭代次数结束</li>
</ol>
<p>用代码实现冷热学习：</p>
<pre><code class="language-python">weight = 0.5
input = 0.5
goal_prediction = 0.8

step_amount = 0.001

for iteration in range(1101):
    prediction = input * weight
    error = (prediction - goal_prediction) ** 2
    
    print(f&quot;Error: {error} Prediction: {prediction}&quot;)
    
    up_prediction = input * (weight + step_amount)
    down_prediction = input * (weight - step_amount)
    
    up_error = (up_prediction - goal_prediction) ** 2
    down_error = (down_prediction - goal_prediction) ** 2
    
    if up_error &gt; down_error:
        weight -= step_amount
    elif up_error &lt; down_error:
        weight += step_amount
</code></pre>
<pre><code class="language-shell">Error: 0.30250000000000005 Prediction: 0.25
Error: 0.3019502500000001 Prediction: 0.2505
Error: 0.30140100000000003 Prediction: 0.251
...
Error: 2.5000000003280753e-07 Prediction: 0.7994999999999672
Error: 1.0799505792475652e-27 Prediction: 0.7999999999999672
</code></pre>
<p>可以看到，在迭代结束后，误差已经极小，预测输出与目标值基本一致。</p>
<p>通过冷热学习可以快速了解神经网络调整过程与最终输出的关系，在深度学习实际应用中并不会使用该方法，主要有如下原因：</p>
<ol>
<li>每次调整权重的过程需要进行多次计算比较，效率较低</li>
<li>有些情况下，迭代中权重调整可能发生震荡，比如权重刚在这次迭代中减了 step_amount, 又在下次迭代中加了 step_amount。这种情况下，权重会在两个值之间一直变化，对应误差也会发生震荡，不会一直下降。</li>
</ol>
<p>接下来，继续了解梯度下降方法，也是深度学习中最常用的方法。</p>
<h4 id="梯度下降">梯度下降</h4>
<p>神经网络前向传播的过程中，只能够通过改变权重来改变预测输出，因此可以根据预测输出与目标值的差距反过来调整权重，使得调整后的权重能够使误差更小。因为预测输出和输入之间存在函数映射关系，因此可以通过求导数的方法确定二者之间的变化关系，这便是梯度下降的核心思想。
<img src="https://yinyu-newbie.github.io/post-images/1561803788367.png" alt=""></p>
<pre><code class="language-python">weight = 0.5
goal_pred = 0.8
input = 0.5

for iteration in range(20):
    pred = input * weight
    error = (pred - goal_pred) ** 2
    direction_and_amount = (pred - goal_pred) * input
    weight -= direction_and_amount
    print(f&quot;Error: {error} Prediction: {pred}&quot;)
</code></pre>
<pre><code class="language-shell">Error: 0.30250000000000005 Prediction: 0.25
Error: 0.17015625000000004 Prediction: 0.3875
...
Error: 9.614592036015323e-06 Prediction: 0.7968992594374867
Error: 5.408208020258491e-06 Prediction: 0.7976744445781151
</code></pre>
<p>当 input 较大时，上述代码中每次更新权重变化可能较大，同样也会发生震荡的情况，导致误差值难以逐渐变小。
<img src="https://yinyu-newbie.github.io/post-images/1561803913979.png" alt="">
<img src="https://yinyu-newbie.github.io/post-images/1561803917861.png" alt="">
为了改善上述情况，在更新权重时可以加入一个 alpha 值来控制权重调整的大小。在深度学习训练神经网络的过程中，这样的参数设置很多，通常都是通过尝试和经验确定这些参数的大小。</p>
<pre><code class="language-python">weight -= alpha * direction_and_amount
</code></pre>
<h3 id="总结">总结</h3>
<ul>
<li>通过误差函数能够计算当前神经网络的表现，并且可以用于后续训练</li>
<li>神经网络的学习即调整权重的过程</li>
<li>神经网络权重的调整通常是通过梯度下降方法进行</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grokking Deep Learning - 前向传播]]></title>
        <id>https://yinyu-newbie.github.io/post/forward_propagation</id>
        <link href="https://yinyu-newbie.github.io/post/forward_propagation">
        </link>
        <updated>2019-06-28T08:46:32.000Z</updated>
        <content type="html"><![CDATA[<p>神经网络的前向传播过程，即在权重的基础上给定输入，得到对应的输出。输入即可以观察到的<strong>信息</strong>，权重即<strong>知识</strong>，输出即最终的预测结果。以足球比赛为例，输入<strong>控球率</strong>，乘以权重，得到<strong>赢球概率</strong>。</p>
<h3 id="单输入单输出的前向传播">单输入单输出的前向传播</h3>
<p>最简单的前向传播神经网络，即输入单个观察值，预测得到单个输出结果。
<img src="https://yinyu-newbie.github.io/post-images/1561720151730.png" alt=""></p>
<pre><code class="language-python">weight = 0.1

def neural_network(input, weight):
    prediction = input * weight
    return prediction

number_of_toes = [8.5, 9.5, 10, 9]
input = number_of_toes[0]
pred = neural_network(input, weight)
print(pred)
</code></pre>
<pre><code class="language-shell">&gt;&gt;&gt; 0.8500000000000001
</code></pre>
<h3 id="多输入单输出的前向传播">多输入单输出的前向传播</h3>
<p>单个输入值情况下，考虑不够全面，纳入更多不同的输入值，最终的输出结果综合考虑了更多因素影响。
<img src="https://yinyu-newbie.github.io/post-images/1561720303209.png" alt=""></p>
<pre><code class="language-python">weights = [0.1, 0.2, 0]

def neural_network(input, weights):
    pred = w_sum(input, weights)
    return pred

def w_sum(a, b):
    assert(len(a) == len(b))
    
    output = 0
    for i in range(len(a)):
        output += (a[i] * b[i])
        
    return output

toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

input = [toes[0], wlrec[0], nfans[0]]

pred = neural_network(input, weights)
print(pred)
</code></pre>
<pre><code class="language-shell">&gt;&gt;&gt; 0.9800000000000001
</code></pre>
<h4 id="numpy实现">Numpy实现</h4>
<p>使用Numpy，可以方便地完成各类矩阵、向量操作。这里可以直接完成向量内积计算。</p>
<pre><code class="language-python">import numpy as np

weights = np.array([0.1, 0.2, 0])

def neural_network(input, weights):
    pred = input.dot(weights)
    return pred

toes = np.array([8.5, 9.5, 9.9, 9.0])
wlrec = np.array([0.65, 0.8, 0.8])
nfans = np.array([1.2, 1.3, 0.5, 1.0])

input = np.array([toes[0], wlrec[0], nfans[0]])
pred = neural_network(input, weights)
print(pred)
</code></pre>
<pre><code class="language-shell">&gt;&gt;&gt; 0.9800000000000001
</code></pre>
<h3 id="单输入多输出的前向传播">单输入多输出的前向传播</h3>
<p>单个输入值可以影响多个不同的输出，对应单输入多输出情况。
<img src="https://yinyu-newbie.github.io/post-images/1561721091245.png" alt=""></p>
<pre><code class="language-python">weights = [0.3, 0.2, 0.9]

def neural_network(input, weights):
    pred = ele_mul(input, weights)
    return pred

def ele_mul(number, vector):
    output = [0, 0, 0]
    assert(len(output) == len(vector))
    
    for i in range(len(vector)):
        output[i] = number * vector[i]
        
    return output

wlrec = [0.65, 0.8, 0.8, 0.9]
input = wlrec[0]
pred = neural_network(input, weights)
print(pred)
</code></pre>
<pre><code class="language-shell">&gt;&gt;&gt; [0.195, 0.13, 0.5850000000000001]
</code></pre>
<h3 id="多输入多输出的前向传播">多输入多输出的前向传播</h3>
<p><img src="https://yinyu-newbie.github.io/post-images/1561721209372.png" alt=""></p>
<pre><code class="language-python">weights = [[0.1, 0.1, -0.3],
           [0.1, 0.2, 0.0],
           [0.0, 1.3, 0.1]]

def neural_network(input, weights):
    pred = vect_mat_mul(input, weights)
    
    return pred

def w_sum(a, b):
    assert(len(a) == len(b))
    
    output = 0
    for i in range(len(a)):
        output += (a[i] * b[i])
    return output

def vect_mat_mul(vect, matrix):
    assert(len(vect) == len(matrix))
 
    output = [0, 0, 0]
    
    for i in range(len(vect)):
        output[i] = w_sum(vect, matrix[i])
        
    return output


toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

input = [toes[0], wlrec[0], nfans[0]]

pred = neural_network(input, weights)
print(pred)
</code></pre>
<pre><code class="language-shell">&gt;&gt;&gt; [0.555, 0.9800000000000001, 0.9650000000000001]
</code></pre>
<p>神经网络就像积木一样，通过堆叠不同的层可以构建更加复杂的神经网络。</p>
<h3 id="基于预测再进行预测">基于预测再进行预测</h3>
<p>在前面多输入多输出的情况下，再接一个一个多输入多输出的网络层，构建一个多层神经网络。
<img src="https://yinyu-newbie.github.io/post-images/1561721619049.png" alt=""></p>
<pre><code class="language-python">ih_wgt = [[0.1, 0.2, -0.1],
          [-0.1, 0.1, 0.9],
          [0.1, 0.4, 0.1]]

hp_wgt = [[0.3, 1.1, -0.3],
          [0.1, 0.2, 0.0],
          [0.0, 1.3, 0.1]]

weights = [ih_wgt, hp_wgt]

def neural_network(input, weights):
    hid = vect_mat_mul(input, weights[0])
    pred = vect_mat_mul(hid, weights[1])
    return pred

toes = [8.5, 9.5, 9.9, 9.0]
wlrec = [0.65, 0.8, 0.8, 0.9]
nfans = [1.2, 1.3, 0.5, 1.0]

input = [toes[0], wlrec[0], nfans[0]]
pred = neural_network(input, weights)
print(pred)
</code></pre>
<pre><code class="language-shell">&gt;&gt;&gt; [0.21350000000000002, 0.14500000000000002, 0.5065]
</code></pre>
<h4 id="numpy-版本">Numpy 版本</h4>
<pre><code class="language-python">import numpy as np

ih_wgt = np.array([[0.1, 0.2, -0.1],
                   [-0.1, 0.1, 0.9],
                   [0.1, 0.4, 0.1]]).T

hp_wgt = np.array([[0.3, 1.1, -0.3],
                   [0.1, 0.2, 0.0],
                   [0.0, 1.3, 0.1]]).T

weights = [ih_wgt, hp_wgt]

def neural_network(input, weights):
    hid = input.dot(weights[0])
    pred = hid.dot(weights[1])
    return pred

toes = np.array([8.5, 9.5, 9.9, 9.0])
wlrec = np.array([0.65, 0.8, 0.8, 0.9])
nfans = np.array([1.2, 1.3, 0.5, 1.0])

input = np.array([toes[0], wlrec[0], nfans[0]])

pred = neural_network(input, weights)
pred
</code></pre>
<pre><code class="language-shell">&gt;&gt;&gt; array([0.2135, 0.145 , 0.5065])
</code></pre>
<h3 id="总结">总结</h3>
<p>前向传播只能通过输入值得到计算后的输出值，但目前存在二个问题:</p>
<ul>
<li>不知道输出结果好坏与否</li>
<li>如果输出结果不理想，如何调整权重
上述问题的解决方法将会在后续提到。</li>
</ul>
]]></content>
    </entry>
</feed>